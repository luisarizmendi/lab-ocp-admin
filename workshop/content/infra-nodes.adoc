## OpenShift Infrastructure Nodes
The OpenShift subscription model allows customers to run various core
infrastructure components at no additional charge. In other words, a node
that is only running core OpenShift infrastructure components is not counted
in terms of the total number of subscriptions required to cover the
environment.

OpenShift components that fall into the infrastructure categorization
include:

* kubernetes and OpenShift control plane services ("masters")
* router
* container image registry
* cluster metrics collection ("monitoring")
* cluster aggregated logging
* service brokers

Any node running a container/pod/component not described above is considered
a worker and must be covered by a subscription.

### Creating an Infra Node

Worker nodes can be given the infra role by labeling them appropriately.

1. Create a default node selector, so pods without nodeSelector will be assigned a subset of nodes to be deployed on, for example by default deploy in worker nodes
2. Add a label to the worker node(s) you wish to act as infra node(s)
3. (optional) Delete the worker label from the infra node(s) so pods assigned to worker nodes are not deployed in infra nodes.
4. Check to see if applicable nodes now have the infra role.
5. Move router pods on the infra node(s)
6. Move registry pods on the infra node(s)
7. Move monitoring pods on the infra node(s)
8. Move logging pods on the infra node(s)
9. Create a new MachineConfigPool so the infra nodes are eligible for updates.

#### Create a default node selector
You can use default node selectors on pods together with labels on nodes to constrain all pods created in a cluster to specific nodes.

With cluster node selectors, when you create a pod in that cluster, OpenShift Container Platform adds the appropriate <key>:<value> and schedules the pod on nodes with matching labels.

If the project where you are creating the pod has a project node selector, that selector takes preference over a cluster node selector.

To add a default cluster node selector:

Edit the Scheduler Operator Custom Resource to add the cluster node selectors:

[source,bash,role="execute"]
----
oc patch scheduler cluster --type='json' -p='[{"op": "add", "path": "/spec/defaultNodeSelector", "value":"node-role.kubernetes.io/worker="}]' 
----

It should look like this:

```YAML
apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  creationTimestamp: "2020-02-03T13:47:21Z"
  generation: 2
  name: cluster
  resourceVersion: "69604"
  selfLink: /apis/config.openshift.io/v1/schedulers/cluster
  uid: a9d5824e-b362-4e08-87d3-51baaf312b86
spec:
  defaultNodeSelector: node-role.kubernetes.io/worker=
  mastersSchedulable: false
  policy:
    name: ""
status: {}
```


After making this change, wait for the pods in the openshift-kube-apiserver project to redeploy. This can take several minutes. The default cluster node selector does not take effect until the pods redeploy.

Check that the worker nodes have such label:

[source,bash,role="execute"]
----
oc get node --show-labels | grep worker
----


#### Add infra label to the worker nodes

Label the nodes that you want to act as infra nodes, for example:

[source,bash,role="copypaste copypaste-warning"]
----
oc label node worker0.baremetalocp.ocp.lablocal node-role.kubernetes.io/infra=""
----


#### optional - Delete the worker label from the infra nodes

If you want that those nodes only run infra-related workloads you can remove the worker label from them, for example:

[source,bash,role="copypaste copypaste-warning"]
----
oc label node worker0.baremetalocp.ocp.lablocal node-role.kubernetes.io/worker-
----


#### Check applicable nodes

[source,bash,role="execute"]
----
oc get nodes
----

#### Move router pods on the infra node(s)


The OpenShift router is managed by an `Operator` called
`openshift-ingress-operator`. Its `Pod` lives in the
`openshift-ingress-operator` project:

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress-operator
----

The actual default router instance lives in the `openshift-ingress` project.  Take a look at the `Pods`.

[source,bash,role="execute"]
----
oc get pods -n openshift-ingress -o wide
----

And you will see something like:

```
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                        NOMINATED NODE
router-default-7bc4c9c5cd-clwqt   1/1     Running   0          9h    10.128.2.7   ip-10-0-144-70.us-east-2.compute.internal   <none>
router-default-7bc4c9c5cd-fq7m2   1/1     Running   0          9h    10.131.0.7   ip-10-0-138-38.us-east-2.compute.internal   <none>
```

Review a `Node` on which a router is running:

[source,bash,role="copypaste copypaste-warning"]
----
oc get node ip-10-0-144-70.us-east-2.compute.internal
----

You will see that it has the role of `worker`.

```
NAME                                        STATUS   ROLES    AGE   VERSION
ip-10-0-144-70.us-east-2.compute.internal   Ready    worker   9h    v1.12.4+509916ce1
```

The default configuration of the router operator is to
pick nodes with the role of `worker`. But, now that we have created dedicated
infrastructure nodes, we want to tell the operator to put the router
instances on nodes with the role of `infra`.

The OpenShift router operator uses a custom resource definition (`CRD`)
called `ingresses.config.openshift.io` to define the default routing
subdomain for the cluster:

[source,bash,role="execute"]
----
oc get ingresses.config.openshift.io cluster -o yaml
----

The `cluster` object is observed by the router operator as well as the
master. Yours likely looks something like:

```YAML
apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  creationTimestamp: 2019-04-08T14:37:49Z
  generation: 1
  name: cluster
  resourceVersion: "396"
  selfLink: /apis/config.openshift.io/v1/ingresses/cluster
  uid: e1ec463c-5a0b-11e9-93e8-028b0fb1636c
spec:
  domain: {{ ROUTE_SUBDOMAIN }}
status: {}
```

Individual router deployments are managed via the
`ingresscontrollers.operator.openshift.io` CRD. There is a default one
created in the `openshift-ingress-operator` namespace:

[source,bash,role="execute"]
----
oc get ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator -o yaml
----

Yours looks something like:

```YAML
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-08T14:46:15Z
  finalizers:
  - ingress.openshift.io/ingress-controller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "2056085"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 0fac160d-5a0d-11e9-a3bb-02d64e703494
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-08T14:47:14Z
    status: "True"
    type: Available
  domain: apps.cluster-f4a3.f4a3.openshiftworkshop.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingress.operator.openshift.io/ingress-controller-deployment=default
```

To specify a `nodeSelector` that tells the router pods to hit the
infrastructure nodes, we can apply the following configuration:



[source,bash,role="execute"]
----
oc patch ingresscontroller/default --type=merge -n openshift-ingress-operator -p '{"spec": {"nodePlacement":{"nodeSelector":{"matchLabels":{"node-role.kubernetes.io/infra": ""}}}}}'
----

Run:

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress -o wide
----

[NOTE]
====
Your session may timeout during the router move. Please refresh the page to get your session back. You will not lose your terminal session but may have to navigate back to this page manually.
====

If you're quick enough, you might catch either `Terminating` or
`ContainerCreating` pods. The `Terminating` pod was running on one of the
worker nodes. The `Running` pods eventually are on one of our nodes with the
`infra` role.



#### Move registry pods on the infra node(s)

The registry uses a similar `CRD` mechanism to configure how the operator
deploys the actual registry pods. That CRD is
`configs.imageregistry.operator.openshift.io`. You will edit the `cluster` CR
object in order to add the `nodeSelector`. First, take a look at it:

[source,bash,role="execute"]
----
oc get configs.imageregistry.operator.openshift.io/cluster -o yaml
----

You will see something like:

```YAML
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2019-08-06T13:57:22Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 2
  name: cluster
  resourceVersion: "13218"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 1cb6272a-b852-11e9-9a54-02fdf1f6ca7a
spec:
  defaultRoute: false
  httpSecret: fff8bb0952d32e0aa56adf0ac6f6cf5267e0627f7b42e35c508050b5be426f8fd5e5108bea314f4291eeacc0b95a2ea9f842b54d7eb61522238f2a2dc471f131
  logging: 2
  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
  storage:
    s3:
      bucket: image-registry-us-east-2-0a598598fc1649d8b96ed91a902b982c-1cbd
      encrypt: true
      keyID: ""
      region: us-east-2
      regionEndpoint: ""
status:
...
```

If you run the following command:

[source,bash,role="execute"]
----
oc patch configs.imageregistry.operator.openshift.io/cluster -p '{"spec":{"nodeSelector":{"node-role.kubernetes.io/infra": ""}}}' --type=merge
----

It will modify the `.spec` of the registry CR in order to add the desired `nodeSelector`.

[NOTE]
====
At this time the image registry is not using a separate project for its
operator. Both the operator and the operand are housed in the
`openshift-image-registry` project.
====

After you run the patch command you should see the registry pod being moved to the
infra node. The registry is in the `openshift-image-registry` project. If you
execute the following quickly enough:

[source,bash,role="execute"]
----
oc get pod -n openshift-image-registry
----

You might see the old registry pod terminating and the new one starting.
Since the registry is being backed by an S3 bucket, it doesn't matter what
node the new registry pod instance lands on. It's talking to an object store
via an API, so any existing images stored there will remain accessible.

Also note that the default replica count is 1. In a real-world environment
you might wish to scale that up for better availability, network throughput,
or other reasons.

If you look at the node on which the registry landed (see the section on the
router), you'll note that it is now running on an infra worker.

Lastly, notice that the `CRD` for the image registry's configuration is not
namespaced -- it is cluster scoped. There is only one internal/integrated
registry per OpenShift cluster.


#### Move monitoring pods on the infra node(s)

The Cluster Monitoring operator is responsible for deploying and managing the
state of the Prometheus+Grafana+AlertManager cluster monitoring stack. It is
installed by default during the initial cluster installation. Its operator
uses a `ConfigMap` in the `openshift-monitoring` project to set various
tunables and settings for the behavior of the monitoring stack.

The following `ConfigMap` definition will configure the monitoring
solution to be redeployed onto infrastructure nodes.

[source,bash,role="execute"]
----
cat <<EOF > monitoring-infra.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
EOF
----

There is no `ConfigMap` created as part of the installation. Without one, the operator will assume
default settings. Verify the `ConfigMap` is not defined in your cluster:

[source,bash,role="execute"]
----
oc get configmap cluster-monitoring-config -n openshift-monitoring
----

You should see:

```
Error from server (NotFound): configmaps "cluster-monitoring-config" not found
```

The operator will, in turn, create several `ConfigMap` objects for the
various monitoring stack components, and you can see them, too:

[source,bash,role="execute"]
----
oc get configmap -n openshift-monitoring
----

You can create the new monitoring config with the following command:

[source,bash,role="execute"]
----
oc create -f monitoring-infra.yaml
----

Watch the monitoring pods move from `worker` to `infra` `Nodes` with:

[source,bash,role="execute"]
----
watch 'oc get pod -n openshift-monitoring'
----

or:

[source,bash,role="execute"]
----
oc get pod -w -n openshift-monitoring
----

#### Move logging pods on the infra node(s)

OpenShift’s log aggregation solution is not installed by default. There is a dedicated lab exercise that goes through the configuration and deployment of logging.


#### Create a new MachineConfigPool

You need to create a MachineConfigPool that contains both the worker role and your custom one as MachineConfig selector. Create a file with the appropiate contents:

[source,bash,role="execute"]
----
cat <<EOF > infra-machineconfigpool.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: infra
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra: ""
EOF
----

Once you have the yaml file, you can create the machineconfigpool using the following command:


[source,bash,role="execute"]
----
oc create -f infra-machineconfigpool.yaml
----

[NOTE]
====
You could lose connectivity to this dashboard since the ingress-controllers will be relocated to the infra nodes. Wait until that happens or monitorize the status with oc get pod -n openshift-ingress

If the workshop is re-deployed (because it was running in a node where workloads are not permited anymore) you will need to re-log again as {{ SYSTEMADMINUSER }} user and use the project app-management
====


Check the machine configs, then you will find that infra config is rendered successfully:


[source,bash,role="execute"]
----
oc get machineconfig
----

You should see something like this:

```
NAME                                                        GENERATEDBYCONTROLLER                      IGNITIONVERSION   CREATED
00-master                                                   25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
00-worker                                                   25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
01-master-container-runtime                                 25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
01-master-kubelet                                           25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
01-worker-container-runtime                                 25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
01-worker-kubelet                                           25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
99-master-53700259-2f56-42a6-b83b-ba0de614b0ed-registries   25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
99-master-ssh                                                                                          2.2.0             16h
99-worker-1134693e-9b7c-4ee2-b59e-4516320722c0-registries   25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
99-worker-ssh                                                                                          2.2.0             16h
rendered-infra-91df30a9f01f025b82df2ffe6f54b6fc             25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             7m19s
rendered-master-f0fefad236cf326ce16748842fd96f74            25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
rendered-worker-91df30a9f01f025b82df2ffe6f54b6fc            25bb6aeb58135c38a667e849edf5244871be4992   2.2.0             16h
```


Now you can create a machine config that uses the custom pool name as the label:


[source,bash,role="execute"]
----
cat <<EOF > infra-machineconfig.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: infra
  name: 51-infra
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:,infra
        filesystem: root
        mode: 0644
        path: /etc/infratest
EOF
----


[source,bash,role="execute"]
----
oc create -f infra-machineconfig.yaml
----

[NOTE]
====
You could lose connectivity to this dashboard since the ingress-controllers will be relocated to the infra nodes. Wait until that happens or monitorize the status with oc get pod -n openshift-ingress

If the workshop is re-deployed (because it was running in a node where workloads are not permited anymore) you will need to re-log again as {{ SYSTEMADMINUSER }} user and use the project app-management
====


#### Check where the routers are running

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress -o wide
----


### Extra Credit
## Quick Operator Background
Operators are just `Pods`. But they are special `Pods`. They are software
that understands how to deploy and manage applications in a Kubernetes
environment. The power of Operators relies on a recent Kubernetes feature
called `CustomResourceDefinitions` (`CRD`). A `CRD` is exactly what it sounds
like. They are a way to define a custom resource which is essentially
extending the Kubernetes API with new objects.

If you wanted to be able to create/read/update/delete `Foo` objects in
Kubernetes, you would create a `CRD` that defines what a `Foo` resource is and how it
works. You can then create `CustomResources` (`CRs`) -- instances of your `CRD`.

With Operators, the general pattern is that an Operator looks at `CRs` for its
configuration, and then it _operates_ on the Kubernetes environment to do
whatever the configuration specifies. Now you will take a look at how some of
the infrastructure operators in OpenShift do their thing.









